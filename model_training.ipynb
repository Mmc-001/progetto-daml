{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb560a95",
   "metadata": {},
   "source": [
    "# Progetto d'esame di Data Analysis in Experimental Physics with Machine Learning\n",
    "Gruppo composto dagli studenti Luca Attinà, Sharis Feriotto e Matteo Marchisio Caprioglio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf32c52",
   "metadata": {},
   "source": [
    "Dataset ipotesi: https://www.kaggle.com/datasets/vipoooool/new-plant-diseases-dataset\n",
    "\n",
    "Questo dataset non va bene perchè ha fatto data aug sul validation dataset, fallback al plant village originale: https://www.tensorflow.org/datasets/catalog/plant_village"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1337f65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f62af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.config.list_physical_devices(''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec33ccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PlantVillage dataset from TFDS instead of the new dataset (it performed data aug on the validation set, which is wrong)\n",
    "(ds_train, ds_val, ds_test), ds_info = tfds.load(\n",
    "    'plant_village',\n",
    "    split=['train[:80%]', 'train[80%:95%]', 'train[95%:]'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,  # returns (image, label) pairs\n",
    "    with_info=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f2ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example from the dataset\n",
    "tfds.show_examples(ds_train, ds_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1abc41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ds_info.features['label'].num_classes\n",
    "print(f\"Number of classes: {class_names}\")\n",
    "class_names = ds_info.features['label'].names\n",
    "print(f\"Class names: {class_names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4057cafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e72308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training dataset to a DataFrame\n",
    "df = tfds.as_dataframe(ds_train, ds_info)\n",
    "\n",
    "# Count the number of samples per class\n",
    "class_counts = df['label'].value_counts().sort_index()\n",
    "\n",
    "# Print the counts with class names\n",
    "for idx, count in class_counts.items():\n",
    "    print(f\"{class_names[idx]}: {count} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9672846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful constants\n",
    "IMG_SIZE = (128, 128)\n",
    "BATCH_SIZE = 128 \n",
    "APPLY_DATA_AUGMENTATION = False\n",
    "N_EPOCHS = 30\n",
    "NUM_CLASSES = ds_info.features['label'].num_classes\n",
    "DROP_RATE = 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64c0060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_augmentation = tf.keras.Sequential([\n",
    "    # tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    # tf.keras.layers.RandomFlip(\"vertical\"),\n",
    "    # tf.keras.layers.RandomRotation(0.1),\n",
    "    # tf.keras.layers.RandomZoom(0.1),\n",
    "    # tf.keras.layers.RandomContrast(0.1),\n",
    "# ])\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = tf.image.resize(image, IMG_SIZE)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, tf.one_hot(label, ds_info.features['label'].num_classes)\n",
    "\n",
    "# def preprocess_with_aug(image, label):\n",
    "    # image = tf.image.resize(image, IMG_SIZE)\n",
    "    # image = data_augmentation(image)  # <-- augment here\n",
    "    # image = tf.cast(image, tf.float32) / 255.0\n",
    "    # return image, tf.one_hot(label, ds_info.features['label'].num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeb63f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and batch the datasets\n",
    "# if APPLY_DATA_AUGMENTATION:\n",
    "    # print(\"Data augmentation is enabled.\")\n",
    "    # train_ds = ds_train.map(preprocess_with_aug, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)  # training dataset with data augmentation\n",
    "# else:\n",
    "    # print(\"Data augmentation is disabled.\")\n",
    "    # train_ds = ds_train.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)  # training dataset without data augmentation\n",
    "\n",
    "train_ds = ds_train.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = ds_val.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds = ds_test.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba012ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, BatchNormalization, Dense, Conv2D, MaxPooling2D, Dropout, Flatten, GlobalAveragePooling2D, ReLU, Rescaling\n",
    "from keras.optimizers.legacy import Adam, SGD\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras import regularizers\n",
    "\n",
    "from keras.metrics import CategoricalAccuracy, Precision, Recall\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b31932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_cnn(#input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3),\n",
    "                num_classes=NUM_CLASSES):\n",
    "    model = Sequential([\n",
    "        # Rescaling(1./255, input_shape=input_shape),\n",
    "        \n",
    "        Conv2D(16, (5, 5), padding='same', kernel_regularizer=regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(DROP_RATE),\n",
    "        \n",
    "        Conv2D(32, (5, 5), padding='same', kernel_regularizer=regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(DROP_RATE),\n",
    "        \n",
    "        Flatten(),\n",
    "        \n",
    "        Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d93c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_cnn_v2(#input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3),\n",
    "                num_classes=NUM_CLASSES):\n",
    "    model = Sequential([\n",
    "        # Rescaling(1./255, input_shape=input_shape),\n",
    "        \n",
    "        Conv2D(16, (5, 5), padding='same', kernel_regularizer=regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(DROP_RATE),\n",
    "        \n",
    "        Conv2D(32, (5, 5), padding='same', kernel_regularizer=regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(DROP_RATE),\n",
    "\n",
    "        Conv2D(64, (5, 5), padding='same', kernel_regularizer=regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(DROP_RATE),\n",
    "\n",
    "        # Flatten(),\n",
    "        GlobalAveragePooling2D(),\n",
    "        \n",
    "        Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "        # Dropout(DROP_RATE),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd33abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simple_cnn_v2()\n",
    "model.build(input_shape=(None, IMG_SIZE[0], IMG_SIZE[1], 3))  # Build the model with dynamic batch size\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f783e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.0002)\n",
    "# optimizer = SGD(learning_rate=0.05, momentum=0.9)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=CategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "check_point_filename = 'best_model.h5'  # Default checkpoint filename for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a31e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Get all labels from the training set\n",
    "labels = []\n",
    "for _, label in ds_train:\n",
    "    labels.append(label.numpy())\n",
    "labels = np.array(labels)\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(labels),\n",
    "    y=labels\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "print(class_weights_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5de7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=N_EPOCHS,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1),\n",
    "        ModelCheckpoint(check_point_filename, monitor='val_loss', save_best_only=True, verbose=1),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2)\n",
    "    ],\n",
    "    class_weight=class_weights_dict,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d786741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c142fee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
