{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb560a95",
   "metadata": {},
   "source": [
    "# Progetto d'esame di Data Analysis in Experimental Physics with Machine Learning\n",
    "Gruppo composto dagli studenti Luca Attinà, Sharis Feriotto e Matteo Marchisio Caprioglio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf32c52",
   "metadata": {},
   "source": [
    "Dataset ipotesi: https://www.kaggle.com/datasets/vipoooool/new-plant-diseases-dataset\n",
    "\n",
    "Questo dataset non va bene perchè ha fatto data aug sul validation dataset, fallback al plant village originale: https://www.tensorflow.org/datasets/catalog/plant_village"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1337f65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f62af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.config.list_physical_devices(''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec33ccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PlantVillage dataset from TFDS instead of the new dataset (it performed data aug on the validation set, which is wrong)\n",
    "(ds_train, ds_val, ds_test), ds_info = tfds.load(\n",
    "    'plant_village',\n",
    "    split=['train[:80%]', 'train[80%:95%]', 'train[95%:]'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,  # returns (image, label) pairs\n",
    "    with_info=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f2ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example from the dataset\n",
    "tfds.show_examples(ds_train, ds_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1abc41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ds_info.features['label'].num_classes\n",
    "print(f\"Number of classes: {class_names}\")\n",
    "class_names = ds_info.features['label'].names\n",
    "print(f\"Class names: {class_names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4057cafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e72308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training dataset to a DataFrame\n",
    "df = tfds.as_dataframe(ds_train, ds_info)\n",
    "\n",
    "# Count the number of samples per class\n",
    "class_counts = df['label'].value_counts().sort_index()\n",
    "\n",
    "# Print the counts with class names\n",
    "for idx, count in class_counts.items():\n",
    "    print(f\"{class_names[idx]}: {count} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9672846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful constants\n",
    "IMG_SIZE = (128, 128)\n",
    "BATCH_SIZE = 64\n",
    "APPLY_DATA_AUGMENTATION = False\n",
    "N_EPOCHS = 30\n",
    "NUM_CLASSES = ds_info.features['label'].num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64c0060",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    # tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    # tf.keras.layers.RandomFlip(\"vertical\"),\n",
    "    tf.keras.layers.RandomRotation(0.1),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "    tf.keras.layers.RandomContrast(0.1),\n",
    "])\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = tf.image.resize(image, IMG_SIZE)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, tf.one_hot(label, ds_info.features['label'].num_classes)\n",
    "\n",
    "def preprocess_with_aug(image, label):\n",
    "    image = tf.image.resize(image, IMG_SIZE)\n",
    "    image = data_augmentation(image)  # <-- augment here\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, tf.one_hot(label, ds_info.features['label'].num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeb63f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and batch the datasets\n",
    "if APPLY_DATA_AUGMENTATION:\n",
    "    print(\"Data augmentation is enabled.\")\n",
    "    train_ds = ds_train.map(preprocess_with_aug).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)  # training dataset with data augmentation\n",
    "else:\n",
    "    print(\"Data augmentation is disabled.\")\n",
    "    train_ds = ds_train.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)  # training dataset without data augmentation\n",
    "\n",
    "val_ds = ds_val.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds = ds_test.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc09d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count samples per class in the training set\n",
    "# def count_classes(dataset, class_names):\n",
    "#     class_counts = [0] * len(class_names)\n",
    "    \n",
    "#     for images, labels in dataset:\n",
    "#         for label in labels:\n",
    "#             class_idx = np.argmax(label.numpy())\n",
    "#             class_counts[class_idx] += 1\n",
    "    \n",
    "#     return class_counts\n",
    "\n",
    "# # Count classes in training set\n",
    "# train_counts = count_classes(train_ds.unbatch(), class_names)\n",
    "\n",
    "# # Display results\n",
    "# print(\"Training set class distribution:\")\n",
    "# for i, (class_name, count) in enumerate(zip(class_names, train_counts)):\n",
    "#     print(f\"{class_name}: {count} images\")\n",
    "\n",
    "# print(f\"\\nTotal training images: {sum(train_counts)}\")\n",
    "# print(f\"Min samples: {min(train_counts)}\")\n",
    "# print(f\"Max samples: {max(train_counts)}\")\n",
    "# print(f\"Average per class: {sum(train_counts)/len(train_counts):.1f}\")\n",
    "# print(f\"Imbalance ratio: {max(train_counts)/min(train_counts):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba012ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, BatchNormalization, Dense, Conv2D, MaxPooling2D, Dropout, Flatten, GlobalAveragePooling2D, ReLU, Rescaling\n",
    "from keras.optimizers.legacy import Adam, SGD\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "\n",
    "from keras.metrics import CategoricalAccuracy, Precision, Recall\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b31932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_cnn(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3), num_classes=NUM_CLASSES):\n",
    "    model = Sequential([\n",
    "        Rescaling(1./255, input_shape=input_shape),\n",
    "        Conv2D(16, (5, 5), activation='relu', padding='same'),\n",
    "        Dropout(0.2),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(32, (5, 5), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd33abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simple_cnn()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f783e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.0002)\n",
    "# optimizer = SGD(learning_rate=0.05, momentum=0.9)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=CategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "if APPLY_DATA_AUGMENTATION:\n",
    "    check_point_filename = 'best_model_w_augmentation.h5'\n",
    "else:\n",
    "    check_point_filename = 'best_model_wo_augmentation.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5de7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=N_EPOCHS,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1),\n",
    "        ModelCheckpoint(check_point_filename, monitor='val_loss', save_best_only=True, verbose=1),\n",
    "        # ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d786741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
