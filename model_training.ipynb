{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb560a95",
   "metadata": {},
   "source": [
    "# Progetto d'esame di Data Analysis in Experimental Physics with Machine Learning\n",
    "Gruppo composto dagli studenti Luca Attinà, Sharis Feriotto e Matteo Marchisio Caprioglio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf32c52",
   "metadata": {},
   "source": [
    "Dataset ipotesi: https://www.kaggle.com/datasets/vipoooool/new-plant-diseases-dataset\n",
    "\n",
    "Questo dataset non va bene perchè ha fatto data aug sul validation dataset, fallback al plant village originale: https://www.tensorflow.org/datasets/catalog/plant_village"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1337f65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('./src')\n",
    "import pickle\n",
    "\n",
    "# from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f62af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315a832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed setting for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "tf.keras.utils.set_random_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd18470",
   "metadata": {},
   "source": [
    "## Load the PlantVillage dataset from TFDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a96649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_val), ds_info = tfds.load(\n",
    "    'plant_village',\n",
    "    split=['train[:80%]', 'train[80%:90%]'],\n",
    "    shuffle_files=[True, False],\n",
    "    as_supervised=True,  # returns (image, label) pairs\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "# print the number of elements in each dataset\n",
    "print(f\"Number of training images: {ds_info.splits['train[:80%]'].num_examples}\")\n",
    "print(f\"Number of validation images: {ds_info.splits['train[80%:95%]'].num_examples}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f2ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example from the dataset\n",
    "SAVE_EXAMPLES = False  # Set to True to save the example images\n",
    "fig = tfds.show_examples(ds_train, ds_info, rows=4, cols=4,)\n",
    "if SAVE_EXAMPLES:\n",
    "    fig.savefig('example_images.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7222498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only labels (not images) for counting\n",
    "labels_list = []\n",
    "for _, label in ds_train:\n",
    "    labels_list.append(label.numpy())\n",
    "\n",
    "# Convert to pandas Series for fast operations\n",
    "labels_series = pd.Series(labels_list)\n",
    "\n",
    "number_of_classes = ds_info.features['label'].num_classes\n",
    "print(f\"Number of classes: {number_of_classes}\")\n",
    "class_names = ds_info.features['label'].names\n",
    "\n",
    "# Count using pandas (fast) but memory-efficient\n",
    "class_counts = labels_series.value_counts().sort_index()\n",
    "\n",
    "# Print the counts with class names\n",
    "print(\"Number of images per class:\")\n",
    "for idx, count in class_counts.items():\n",
    "    print(f\"{class_names[idx]}: {count} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64ff684",
   "metadata": {},
   "source": [
    "### Visualize the class distribution in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ddf9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALIZE = False  # Set to True to normalize the counts\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "alphas = np.linspace(1, 0.4, ds_info.features['label'].num_classes)\n",
    "for (name, count, alpha) in zip(class_names, class_counts, alphas):\n",
    "    plt.bar(name, count / (np.sum(class_counts) if NORMALIZE else 1), color='green', alpha=alpha)\n",
    "plt.xlabel('Class')\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(axis='y')\n",
    "if NORMALIZE:\n",
    "    plt.ylabel('Normalized #Images per Class')\n",
    "    plt.title('Normalized Class Distribution in PlantVillage Dataset')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Add this line to leave space for the title\n",
    "    plt.savefig('norm_class_distr.png')\n",
    "else:\n",
    "    plt.ylabel('#Images per Class')\n",
    "    plt.title('Class Distribution in PlantVillage Dataset')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Add this line to leave space for the title\n",
    "    plt.savefig('class_distr.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9672846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup constants\n",
    "NUM_CLASSES         = ds_info.features['label'].num_classes\n",
    "\n",
    "ARCH_CHOICE         = 0 # 0: simple_cnn_v0, 1: simple_cnn_v1, 2: simple_cnn_v2, 3: simple_cnn_v3\n",
    "\n",
    "WEIGHTS_DIR = './weights'\n",
    "HISTORY_DIR = './train-hist'\n",
    "os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
    "os.makedirs(HISTORY_DIR, exist_ok=True)\n",
    "CHECKPOINT_FILE = f'{WEIGHTS_DIR}/best_model_v{ARCH_CHOICE}.h5'\n",
    "HISTORY_FILE = f'{HISTORY_DIR}/history_v{ARCH_CHOICE}'\n",
    "\n",
    "IMG_SIZE            = (128, 128) # Image size for resizing in preprocessing\n",
    "IMG_CHANNELS        = 3          # Number of channels in the images (RGB)\n",
    "BATCH_SIZE          = 64         # Batch size for training\n",
    "N_EPOCHS            = 50         # Number of epochs for training\n",
    "STARTING_LR         = 0.0002     # Starting learning rate for the optimizer\n",
    "EARLY_STOP_PATIENCE = 4          # Patience for early stopping\n",
    "REDUCE_LR_PATIENCE  = 3          # Patience for reducing learning rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5cd3c7",
   "metadata": {},
   "source": [
    "## Preprocess and batch the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeb63f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import preprocess\n",
    "\n",
    "train_ds = ds_train.map(lambda image, label: preprocess(image, label, ds_info, IMG_SIZE), num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds   = ds_val.map  (lambda image, label: preprocess(image, label, ds_info, IMG_SIZE), num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8f78cd",
   "metadata": {},
   "source": [
    "## Build the architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba012ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Activation, BatchNormalization, Dense, Conv2D, MaxPooling2D, Dropout, Flatten, GlobalAveragePooling2D, ReLU, Rescaling, Add \n",
    "from keras.optimizers.legacy import Adam, SGD\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from keras.applications import ResNet50, InceptionV3, VGG16, MobileNetV2, EfficientNetB0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573cde1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_cnn_v0(input_shape=(IMG_SIZE[0], IMG_SIZE[1], IMG_CHANNELS),\n",
    "                   num_classes=NUM_CLASSES,\n",
    "                   drop_rate=0.4,\n",
    "                #    l2_reg=0.005,\n",
    "                   ):\n",
    "    model = Sequential([\n",
    "        Conv2D(16, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        Dropout(drop_rate),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d3602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_cnn_v1(input_shape=(IMG_SIZE[0], IMG_SIZE[1], IMG_CHANNELS),\n",
    "                  num_classes=NUM_CLASSES,\n",
    "                  drop_rate=0.4,\n",
    "                  #l2_reg=0.005,\n",
    "                   ):\n",
    "    model = Sequential([\n",
    "        Conv2D(16, (3, 3), padding='same', input_shape=input_shape),\n",
    "        Activation('relu'),\n",
    "        Dropout(drop_rate),\n",
    "        MaxPooling2D((2, 2)),\n",
    "\n",
    "        Conv2D(32, (3, 3), padding='same'),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(drop_rate),\n",
    "\n",
    "        Conv2D(64, (3, 3), padding='same'),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(drop_rate),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4a8a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_cnn_v2(input_shape=(IMG_SIZE[0], IMG_SIZE[1], IMG_CHANNELS),\n",
    "                  num_classes=NUM_CLASSES,\n",
    "                  drop_rate=0.4,\n",
    "                  l2_reg=0.005,\n",
    "                  ):\n",
    "    model = Sequential([\n",
    "        Conv2D(16, (3, 3), padding='same', kernel_regularizer=l2(l2_reg), input_shape=input_shape),\n",
    "        Activation('relu'),\n",
    "        Dropout(drop_rate),\n",
    "        MaxPooling2D((2, 2)),\n",
    "\n",
    "        Conv2D(32, (3, 3), padding='same', kernel_regularizer=l2(l2_reg)),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(drop_rate),\n",
    "\n",
    "        Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(l2_reg)),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(drop_rate),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(num_classes, activation='softmax', kernel_regularizer=l2(l2_reg))\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d594cc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_cnn_v3(input_shape=(IMG_SIZE[0], IMG_SIZE[1], IMG_CHANNELS),\n",
    "                  num_classes=NUM_CLASSES,\n",
    "                  drop_rate=0.4,\n",
    "                  #l2_reg=0.005,\n",
    "                   ):\n",
    "    model = Sequential([\n",
    "        Conv2D(16, (3, 3), padding='same', input_shape=input_shape),\n",
    "        Activation('relu'),\n",
    "        Dropout(drop_rate),\n",
    "        MaxPooling2D((2, 2)),\n",
    "\n",
    "        Conv2D(32, (3, 3), padding='same'),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(drop_rate),\n",
    "\n",
    "        Conv2D(64, (3, 3), padding='same'),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(drop_rate),\n",
    "\n",
    "        Conv2D(128, (3, 3), padding='same'),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(drop_rate),\n",
    "\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd33abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = [simple_cnn_v0,\n",
    "               simple_cnn_v1,\n",
    "               simple_cnn_v2,\n",
    "               simple_cnn_v3,\n",
    "               ]\n",
    "if ARCH_CHOICE not in range(0, len(models_list) + 1):\n",
    "    raise ValueError(\"Invalid architecture choice\")\n",
    "else:\n",
    "    model = models_list[ARCH_CHOICE]()\n",
    "\n",
    "model.build(input_shape=(None, IMG_SIZE[0], IMG_SIZE[1], IMG_CHANNELS))  # Build the model with dynamic batch size\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f783e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.0002)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    # loss=CategoricalCrossentropy(label_smoothing=0.1),\n",
    "    loss=CategoricalCrossentropy(),\n",
    "    # loss=tf.keras.losses.CategoricalFocalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347f1009",
   "metadata": {},
   "source": [
    "### Calculate class weights for weighted training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a31e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Get all labels from the training set\n",
    "labels = []\n",
    "for _, label in ds_train:\n",
    "    labels.append(label.numpy())\n",
    "labels = np.array(labels)\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(labels),\n",
    "    y=labels\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "class_weights_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2940fa98",
   "metadata": {},
   "source": [
    "## Train the selected architecture\n",
    "Or just visualize the training history of an already-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca011b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy class to mimic Keras History for plotting\n",
    "class DummyHistory:\n",
    "    def __init__(self, history_dict):\n",
    "        self.history = history_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5de7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True  # Change to False to only visualize an already trained model's history\n",
    "\n",
    "if TRAIN:\n",
    "    # Train the model\n",
    "    print(f\"Training the model v{ARCH_CHOICE}...\")\n",
    "    history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=N_EPOCHS,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='val_accuracy',\n",
    "                    #   min_delta=0.005,\n",
    "                      patience=EARLY_STOP_PATIENCE,\n",
    "                      restore_best_weights=True,\n",
    "                      verbose=1),\n",
    "        ModelCheckpoint(CHECKPOINT_FILE,\n",
    "                        # min_delta=0.005,\n",
    "                        monitor='val_accuracy',\n",
    "                        save_best_only=True,\n",
    "                        verbose=1),\n",
    "        ReduceLROnPlateau(monitor='val_accuracy',\n",
    "                        #   min_delta=0.005,\n",
    "                          factor=0.2,\n",
    "                          patience=REDUCE_LR_PATIENCE,\n",
    "                          verbose=1)\n",
    "    ],\n",
    "    class_weight=class_weights_dict,\n",
    "    )\n",
    "    # Save history after training\n",
    "    with open(HISTORY_FILE+'.pkl', 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "else:\n",
    "    # Load history if not training\n",
    "    with open(HISTORY_FILE+'.pkl', 'rb') as f:\n",
    "        loaded_history = pickle.load(f)\n",
    "        history = DummyHistory(loaded_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bbd110",
   "metadata": {},
   "source": [
    "### Visualize the model training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c142fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting import plot_model_history\n",
    "\n",
    "print(f\"Plotting model history for architecture v{ARCH_CHOICE}...\")\n",
    "plot_model_history(history)\n",
    "plt.savefig(HISTORY_FILE+'.png', dpi=400, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7c5856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bdd930",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
