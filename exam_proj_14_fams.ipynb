{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "eb560a95",
      "metadata": {
        "id": "eb560a95"
      },
      "source": [
        "# Progetto d'esame di Data Analysis in Experimental Physics with Machine Learning\n",
        "\n",
        "Gruppo composto dagli studenti Luca Attinà, Sharis Feriotto e Matteo Marchisio Caprioglio"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bf32c52",
      "metadata": {
        "id": "7bf32c52"
      },
      "source": [
        "Dataset ipotesi: https://www.kaggle.com/datasets/vipoooool/new-plant-diseases-dataset\n",
        "Questo dataset non va bene perchè ha fatto data aug sul validation dataset, fallback al plant village originale: https://www.tensorflow.org/datasets/catalog/plant_village"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1337f65a",
      "metadata": {
        "id": "1337f65a"
      },
      "outputs": [],
      "source": [
        "# libraries and packages import\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, BatchNormalization, Activation, MaxPooling2D, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.losses import CategoricalCrossentropy, CategoricalFocalCrossentropy\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import regularizers\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, roc_auc_score, classification_report\n",
        "import seaborn as sns\n",
        "from sklearn.utils import class_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e9c068d",
      "metadata": {
        "id": "1e9c068d"
      },
      "outputs": [],
      "source": [
        "# Seed setting for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "tf.keras.utils.set_random_seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "787d4ff7",
      "metadata": {
        "id": "787d4ff7"
      },
      "outputs": [],
      "source": [
        "# Colab optional setup\n",
        "'''\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "print(\"Running on Colab:\", IS_COLAB)\n",
        "if IS_COLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive/', force_remount=True)\n",
        "  #Adapt the folder to your specific one where you have downloaded the code\n",
        "  %cd /content/drive/My Drive/path_to/exam-project\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0bb7eda",
      "metadata": {
        "id": "b0bb7eda"
      },
      "source": [
        "# Data download and preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d3d1671",
      "metadata": {
        "id": "0d3d1671"
      },
      "source": [
        "Be careful if you are running this on COLAB or locally.\n",
        "Due to some bugs, datasets creation is different.\n",
        "Change the lower COLAB bool to True if running on COLAB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59ed4f2b",
      "metadata": {
        "id": "59ed4f2b"
      },
      "outputs": [],
      "source": [
        "COLAB = False  # if True, run on Google Colab, else on local repository\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e06e1eb0",
      "metadata": {
        "id": "e06e1eb0"
      },
      "outputs": [],
      "source": [
        "# useful constants\n",
        "IMG_SIZE = (128, 128)\n",
        "BATCH_SIZE = 32\n",
        "N_EPOCHS = 30\n",
        "VERBOSE = True # True for debug prints\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1542d0cd",
      "metadata": {
        "id": "1542d0cd"
      },
      "source": [
        "IF RUNNING ON COLAB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8468afb",
      "metadata": {
        "id": "b8468afb"
      },
      "outputs": [],
      "source": [
        "# Load the PlantVillage dataset from TFDS instead of the new dataset (it performed data aug on the validation set, which is wrong)\n",
        "# Only working on Colab (and locally if tfds.load works correctly)\n",
        "\n",
        "def load_from_tfds():\n",
        "    (ds_train, ds_val, ds_test), ds_info = tfds.load(\n",
        "        'plant_village',\n",
        "        split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
        "        shuffle_files=True,\n",
        "        as_supervised=True,  # returns (image, label) pairs\n",
        "        with_info=True,\n",
        "    )\n",
        "\n",
        "    # labels are plant families\n",
        "    class_names = ds_info.features['label'].names\n",
        "    families = sorted({n.split('___')[0] for n in class_names})\n",
        "    split_labels = families\n",
        "    family_map = tf.constant([families.index(n.split('___')[0]) for n in class_names], dtype=tf.int32)\n",
        "\n",
        "    # returns label as one-hot\n",
        "    def to_ohe(img, lbl):\n",
        "      idx = tf.gather(family_map, lbl)\n",
        "      return img, tf.one_hot(idx, len(split_labels))\n",
        "\n",
        "    ds_train = ds_train.map(to_ohe)\n",
        "    ds_val   = ds_val.map(to_ohe)\n",
        "    ds_test  = ds_test.map(to_ohe)\n",
        "\n",
        "    print(split_labels)\n",
        "\n",
        "    return ds_train, ds_val, ds_test, split_labels, ds_info\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd7b4c8b",
      "metadata": {
        "id": "dd7b4c8b"
      },
      "source": [
        "IF RUNNING ON LOCAL REPOSITORY"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d145fba1",
      "metadata": {
        "id": "d145fba1"
      },
      "source": [
        "If you are working on a local repository, first of all you need to clone the dataset into a local folder.\n",
        "Execute the command \"git clone https://github.com/spMohanty/PlantVillage-Dataset\" in the terminal while you are in a known path. Make sure to set the base_path variable below to point to that cloned folder.\n",
        "\n",
        "Only run the git clone command if you haven’t already downloaded the dataset to your PC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80870677",
      "metadata": {
        "id": "80870677"
      },
      "outputs": [],
      "source": [
        "#define prerpocess function\n",
        "def preprocess(image, label, image_size=(128, 128)):\n",
        "    image = tf.image.resize(image, image_size)\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60f96278",
      "metadata": {
        "id": "60f96278"
      },
      "outputs": [],
      "source": [
        "# Define local path to generate split\n",
        "if not COLAB:\n",
        "    from pathlib import Path\n",
        "\n",
        "    base_path = Path(r\"D:\\progetto-daml\") #Change according to the path where PlantVillage-Dataset is cloned.\n",
        "    base_path = base_path / \"PlantVillage-Dataset\"\n",
        "\n",
        "    OUTPUT_ROOT = base_path / \"by_family\" # 14 families splitting\n",
        "    DS_DIR = OUTPUT_ROOT / \"train\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04c93b16",
      "metadata": {
        "id": "04c93b16"
      },
      "outputs": [],
      "source": [
        "#ONLY RUN FIRST TIME, AFTER DATASET CLONING\n",
        "'''\n",
        "SOURCE_DIR = base_path / \"raw/color\"\n",
        "TRAIN_FRAC = 0.8 #change fractions\n",
        "VAL_FRAC = 0.1\n",
        "TEST_FRAC = 0.1\n",
        "\n",
        "# Create output folders\n",
        "for split in (\"train\",\"val\",\"test\"):\n",
        "    folder = OUTPUT_ROOT / split\n",
        "    if folder.exists():\n",
        "        shutil.rmtree(folder) #remove pre-existing folder for new split\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Collect folders name and map into family name folders\n",
        "disease_folders = sorted([d for d in SOURCE_DIR.iterdir() if d.is_dir()])\n",
        "families = sorted({d.name.split(\"___\")[0] for d in disease_folders})\n",
        "\n",
        "for split in (\"train\",\"val\",\"test\"):\n",
        "    for fam in families:\n",
        "        (OUTPUT_ROOT/ split / fam).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Splits files into train, validation and test for each family\n",
        "for disease_dir in disease_folders:\n",
        "    fam = disease_dir.name.split(\"___\")[0]\n",
        "    images = list(disease_dir.glob(\"*.*\"))  # all image files\n",
        "    random.shuffle(images)\n",
        "\n",
        "    n = len(images)\n",
        "    n_train = int(n * TRAIN_FRAC)\n",
        "    n_val   = int(n * VAL_FRAC)\n",
        "\n",
        "    train_imgs = images[:n_train]\n",
        "    val_imgs   = images[n_train:n_train+n_val]\n",
        "    test_imgs  = images[n_train+n_val:]\n",
        "\n",
        "    # Put the datasets into the local folders\n",
        "    for img in train_imgs:\n",
        "        shutil.copy(img, OUTPUT_ROOT/\"train\"/fam/img.name)\n",
        "    for img in val_imgs:\n",
        "        shutil.copy(img, OUTPUT_ROOT/\"val\"/fam/img.name)\n",
        "    for img in test_imgs:\n",
        "        shutil.copy(img, OUTPUT_ROOT/\"test\"/fam/img.name)\n",
        "\n",
        "print(\"Datasets paths are:\\n\",\n",
        "    OUTPUT_ROOT / \"train\\n\",\n",
        "    OUTPUT_ROOT / \"val\\n\",\n",
        "    OUTPUT_ROOT / \"test\\n\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48a4e3fb",
      "metadata": {
        "id": "48a4e3fb"
      },
      "outputs": [],
      "source": [
        "# read split data into train, validation e test sets\n",
        "def prepare_local_dataset():\n",
        "    split_labels = sorted([p.name for p in (OUTPUT_ROOT/\"train\").iterdir() if p.is_dir()]) #folders names\n",
        "\n",
        "    ds_train_ohe = tf.keras.utils.image_dataset_from_directory(\n",
        "        str(OUTPUT_ROOT/\"train\"),\n",
        "        image_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        label_mode='categorical',\n",
        "    )\n",
        "\n",
        "    ds_val_ohe = tf.keras.utils.image_dataset_from_directory(\n",
        "        str(OUTPUT_ROOT/\"val\"),\n",
        "        image_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        label_mode='categorical',\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    ds_test_ohe = tf.keras.utils.image_dataset_from_directory(\n",
        "        str(OUTPUT_ROOT/\"test\"),\n",
        "        image_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        label_mode='categorical',\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    return ds_train_ohe, ds_val_ohe, ds_test_ohe, split_labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d240cad2",
      "metadata": {
        "id": "d240cad2"
      },
      "source": [
        "# Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e176ea8a",
      "metadata": {
        "id": "e176ea8a"
      },
      "outputs": [],
      "source": [
        "# cnn layers\n",
        "def simple_cnn(input_shape, num_classes, l2_coef=1e-3):\n",
        "    model = Sequential([\n",
        "        Conv2D(16, (3, 3), padding='same', kernel_regularizer=regularizers.l2(l2_coef), input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        Activation('relu'),\n",
        "        Dropout(0.4),\n",
        "\n",
        "        Conv2D(32, (3, 3), padding='same', kernel_regularizer=regularizers.l2(l2_coef)),\n",
        "        BatchNormalization(),\n",
        "        Activation('relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.4),\n",
        "\n",
        "        Flatten(),\n",
        "        Dense(num_classes, activation='softmax', kernel_regularizer=regularizers.l2(l2_coef))\n",
        "    ])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2ad89f8",
      "metadata": {
        "id": "f2ad89f8"
      },
      "outputs": [],
      "source": [
        "# preprocess and batch datasets. Useful objects are defined\n",
        "def model_preprocess(COLAB):\n",
        "    if COLAB:\n",
        "        ds_train, ds_val, ds_test, split_labels, ds_info = load_from_tfds()\n",
        "        ds_train = ds_train.shuffle(buffer_size=5000) # only shuffle train set\n",
        "        ds_train = ds_train.map(lambda img, lbl: preprocess(img, lbl, IMG_SIZE)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)  # training dataset without data augmentation\n",
        "        ds_val = ds_val.map(lambda img, lbl: preprocess(img, lbl, IMG_SIZE)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "        ds_test = ds_test.map(lambda img, lbl: preprocess(img, lbl, IMG_SIZE)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "    else:\n",
        "        ds_train, ds_val, ds_test, split_labels = prepare_local_dataset()\n",
        "\n",
        "    num_classes = len(split_labels)\n",
        "    checkpoint = \"best_model_14_families_exam.h5\"\n",
        "\n",
        "    return ds_train, ds_val, ds_test, split_labels, num_classes, checkpoint\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e143c628",
      "metadata": {
        "id": "e143c628"
      },
      "outputs": [],
      "source": [
        "# model compilation\n",
        "def model_train(num_classes):\n",
        "    model = simple_cnn(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3), num_classes=num_classes)\n",
        "\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        #loss=CategoricalFocalCrossentropy(alpha = 0.25, gamma = 2),\n",
        "        loss=CategoricalCrossentropy(),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    model.build(input_shape=(None, IMG_SIZE[0], IMG_SIZE[1], 3))  # Build the model with dynamic batch size\n",
        "    model.summary()\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f115290c",
      "metadata": {
        "id": "f115290c"
      },
      "outputs": [],
      "source": [
        "# program running (all the functions above are executed)\n",
        "train_set, val_set, test_set, split_labels, num_classes, checkpoint_file = model_preprocess(COLAB)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcc2db6e",
      "metadata": {
        "id": "dcc2db6e"
      },
      "outputs": [],
      "source": [
        "# debug prints: labels, number of labels, checkpoint file name\n",
        "if VERBOSE:\n",
        "    print(split_labels)\n",
        "    print(len(split_labels))\n",
        "    print(checkpoint_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09d23531",
      "metadata": {
        "id": "09d23531"
      },
      "outputs": [],
      "source": [
        "# debug print for families - ohe labels cross check\n",
        "def display_samples_with_labels_ohe(dataset, num_samples=5):\n",
        "    plt.figure(figsize=(6, 3 * num_samples))\n",
        "    for i, (img, label_ohe) in enumerate(dataset.unbatch().take(num_samples)):\n",
        "        # Tensors are converted into numpy arrays\n",
        "        img_np   = img.numpy().astype(\"uint8\")\n",
        "        ohe_vec  = label_ohe.numpy()                     # one-hot vector\n",
        "        fam_idx  = int(tf.argmax(label_ohe).numpy())     # family index\n",
        "        fam_name = split_labels[fam_idx]\n",
        "\n",
        "        ax = plt.subplot(num_samples, 1, i + 1)\n",
        "        plt.imshow(img_np)\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(f\"Family: {fam_name} (idx={fam_idx})\\nOHE: {ohe_vec.tolist()}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# example on train batch\n",
        "if VERBOSE:\n",
        "    display_samples_with_labels_ohe(train_set, num_samples=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a60009f8",
      "metadata": {
        "id": "a60009f8"
      },
      "outputs": [],
      "source": [
        "# counting number of images per class\n",
        "from collections import Counter\n",
        "\n",
        "if VERBOSE:\n",
        "    counts = Counter()\n",
        "\n",
        "    for _, batch_labels in train_set:\n",
        "        idxs = np.argmax(batch_labels.numpy(), axis=1)\n",
        "        counts.update(idxs)\n",
        "\n",
        "    print(f\"Number of classes: {len(split_labels)}\")\n",
        "    print(\"Number of images per class:\")\n",
        "    for idx in range(len(split_labels)):\n",
        "        print(f\"{split_labels[idx]}: {counts[idx]} images\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba8dee25",
      "metadata": {
        "id": "ba8dee25"
      },
      "outputs": [],
      "source": [
        "# Visualize the class numbers distribution\n",
        "if VERBOSE:\n",
        "    counts_list = [counts[i] for i in range(len(split_labels))]\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar(split_labels, counts_list, color='skyblue')\n",
        "    plt.xlabel('Family')\n",
        "    plt.ylabel('Number of Images')\n",
        "    plt.title('Classes Distribution')\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.grid(axis='y')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6790890",
      "metadata": {
        "id": "a6790890"
      },
      "outputs": [],
      "source": [
        "# Print element format\n",
        "if VERBOSE:\n",
        "    print(\"Element spec:\", train_set.element_spec)\n",
        "\n",
        "    # Example on a batch\n",
        "    for batch in train_set.take(1):\n",
        "        x, y = batch\n",
        "        print(\"x shape:\", x.shape, \"  dtype:\", x.dtype)\n",
        "        print(\"y shape:\", y.shape, \"  dtype:\", y.dtype)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a467abad",
      "metadata": {
        "id": "a467abad"
      },
      "outputs": [],
      "source": [
        "model = model_train(num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e791278",
      "metadata": {
        "id": "4e791278"
      },
      "outputs": [],
      "source": [
        "#label conversion from ohe to index\n",
        "all_labels = []\n",
        "for batch in train_set:\n",
        "    images, labels_ohe = batch\n",
        "    for lab in labels_ohe:\n",
        "        all_labels.append(int(np.argmax(lab)))\n",
        "\n",
        "all_labels = np.array(all_labels)\n",
        "classes=np.unique(all_labels)\n",
        "\n",
        "# weights calculation (total examples number / (classes number * example in ith class) )\n",
        "weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=classes,\n",
        "    y=all_labels\n",
        ")\n",
        "\n",
        "class_w = dict(zip(classes, weights))\n",
        "\n",
        "# print family name and corresponding weight\n",
        "if VERBOSE:\n",
        "    for idx, w in class_w.items():\n",
        "        name = split_labels[idx]\n",
        "        print(f\"{name:15s}: {w:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "472245eb",
      "metadata": {
        "id": "472245eb"
      },
      "outputs": [],
      "source": [
        "# Model training - Saving best model\n",
        "history = model.fit(\n",
        "    train_set,\n",
        "    validation_data=val_set,\n",
        "    epochs=N_EPOCHS,\n",
        "    #class_weight=class_w, #optional: training with weighted classes\n",
        "    callbacks=[\n",
        "        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1),\n",
        "        ModelCheckpoint(checkpoint_file, monitor='val_loss', save_best_only=True, verbose=1),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edba3ea7",
      "metadata": {
        "id": "edba3ea7"
      },
      "source": [
        "# CNN training history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d786741",
      "metadata": {
        "id": "1d786741"
      },
      "outputs": [],
      "source": [
        "# plot training history (Loss and Accuracy)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qwfq1nl2wreb",
      "metadata": {
        "id": "qwfq1nl2wreb"
      },
      "source": [
        "# Evaluation code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56f83097",
      "metadata": {
        "id": "56f83097"
      },
      "source": [
        "The following code is left here as a backup in case of problem with the compilation wit model_evaluation.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rVyQkS5-vsyq",
      "metadata": {
        "id": "rVyQkS5-vsyq"
      },
      "outputs": [],
      "source": [
        "# Generate Predictions on Test Set\n",
        "from tensorflow.keras import Sequential\n",
        "\n",
        "model = keras.models.load_model('best_model_14_families_focal.h5', compile=False) # insert file name\n",
        "\n",
        "y_true = [] # ground truth\n",
        "y_pred = [] # prediction\n",
        "y_score = [] # predictions vector\n",
        "for images, labels in test_set:\n",
        "    y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
        "    preds = model.predict(images)\n",
        "    y_pred.extend(np.argmax(preds, axis=1))\n",
        "    y_score.append(preds)\n",
        "y_score = np.concatenate(y_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0560fa0f",
      "metadata": {
        "id": "0560fa0f"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "f = h5py.File('best_model_14_families_focal.h5', 'r')\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rjcjWjN5v6Ks",
      "metadata": {
        "id": "rjcjWjN5v6Ks"
      },
      "outputs": [],
      "source": [
        "# Calculate Evaluation Metrics (Accuracy, Precision, Recall, F1)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='weighted')\n",
        "recall = recall_score(y_true, y_pred, average='weighted')\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "print(f\"Accuracy: {accuracy:.6f}\")\n",
        "print(f\"Precision: {precision:.6f}\")\n",
        "print(f\"Recall: {recall:.6f}\")\n",
        "print(f\"F1-score: {f1:.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xA61wCLdv9Dq",
      "metadata": {
        "id": "xA61wCLdv9Dq"
      },
      "outputs": [],
      "source": [
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
        "\n",
        "#Plot confusion matrix\n",
        "plt.figure(figsize=(14, 12))\n",
        "sns.heatmap(cm, annot=False, fmt='d', cmap='viridis', xticklabels=split_labels, yticklabels=split_labels)\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.title('Confusion Matrix (Test Set)')\n",
        "plt.xticks(rotation=90)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SQiJREbjwGl-",
      "metadata": {
        "id": "SQiJREbjwGl-"
      },
      "outputs": [],
      "source": [
        "# Plot ROC Curves for Each Class\n",
        "n_classes = y_score.shape[1]\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(np.eye(n_classes)[y_true][:, i], y_score[:, i])\n",
        "    roc_auc[i] = roc_auc_score(np.eye(n_classes)[y_true][:, i], y_score[:, i])\n",
        "plt.figure(figsize=(12, 12))\n",
        "auc_and_idx = sorted([(roc_auc[i], i) for i in range(n_classes)], reverse=True)\n",
        "for auc, i in auc_and_idx:\n",
        "    plt.plot(fpr[i], tpr[i], label=f'{split_labels[i]} (AUC = {auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], 'r--', lw=2, label='Random Classifier (AUC = 0.5)')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - One vs Rest (Test Set)')\n",
        "plt.legend(fontsize='small', bbox_to_anchor=(1.05, 1), loc='best')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iVQ1cfCGwI1P",
      "metadata": {
        "id": "iVQ1cfCGwI1P"
      },
      "outputs": [],
      "source": [
        "# Display Classification Report\n",
        "report = classification_report(y_true, y_pred, target_names=split_labels)\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c61c6780",
      "metadata": {
        "id": "c61c6780"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
